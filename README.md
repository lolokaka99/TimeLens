<p align="center">
  <h1 align="center">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</h1>
</p>

<p align="center">
  <a href="https://home.j-zh.top/">Jun Zhang</a>, <a href="http://ttengwang.com/">Teng Wang</a>, <a href="https://geyuying.github.io/">Yuying Ge</a>, <a href="https://geyixiao.com/">Yixiao Ge</a>, <a href="https://scholar.google.com/citations?user=evR3uR0AAAAJ">Xinhao Li</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>, <a href="https://scholar.google.com/citations?user=HEuN8PcAAAAJ&hl=en">Limin Wang</a>
</p>

<p align="center">
    &nbsp&nbspğŸ“‘ <a href="TODO"><b>Paper</b></a>&nbsp&nbsp | &nbsp&nbspğŸ  <a href="https://timelens-arc-lab.github.io/"><b>Project Page</b></a>&nbsp&nbsp | ğŸ¤— <a href="https://huggingface.co/collections/TencentARC/timelens"><b>Model & Data</b></a>&nbsp&nbsp | ğŸ† <a href="https://timelens-arc-lab.github.io/#leaderboard"><b>TimeLens-Bench Leaderboard</b></a>&nbsp&nbsp
</p>

## ğŸ”¥ Highlights
- [TimeLens-Bench](https://huggingface.co/datasets/TencentARC/TimeLens-Bench): a comprehensive, high-quality evaluation benchmark for video temporal grounding, consisting of Charades-TimeLens, ActivityNet-TimeLens and QVHighlights-TimeLens.
- [TimeLens-100K](https://huggingface.co/datasets/TencentARC/TimeLens-100K): a large-scale, diverse, high-quality training dataset for video temporal grounding, annotated with Gemini-2.5-Pro.
- [TimeLens Models](#-timelens-models): State-of-the-art open-source models for video temporal grounding.

## ğŸ“¦ Installation

Clone this repository and navigate to the folder
```bash
git clone https://github.com/TencentARC/TimeLens.git
cd TimeLens
```

Create a Conda environment and install the required packages
```bash
conda create -n timelens python=3.11 -y
conda activate timelens
pip install -r requirements.txt -f https://download.pytorch.org/whl/cu124 # We use CUDA Version 12.4
pip install flash-attn==2.7.4.post1 --no-build-isolation --no-cache-dir
```

## ğŸ¤– Using TimeLens Models
TimeLens models are a family of MLLMs with SotA video temporal grounding performance. They are built upon the Qwen2.5-VL and Qwen3-VL baselines through training on our high-quality [TimeLens-100K](#ï¸-training-on-timelens-100k) dataset, leveraging our carefully crafted RLVR (reinforcement learning with verifiable rewards) recipe and improved timestamp encoding strategy.

### ğŸš€ Quick Start
All models are available on Hugging Face and support out-of-the-box inference using the ğŸ¤—Transformers library. For detailed usage instructions and code examples, please refer to the specific model's Hugging Face page linked below.

### ğŸ† Model Zoo & Performance
The following table lists our models with their Hugging Face links and grounding performance:

<table>
  <thead>
    <tr>
      <th rowspan="2" align="center">Model <br>(with ğŸ¤—HuggingFace Link)</th>
      <th colspan="4" align="center">Charades-TimeLens</th>
      <th colspan="4" align="center">ActivityNet-TimeLens</th>
      <th colspan="4" align="center">QVHighlights-TimeLens</th>
    </tr>
    <tr>
      <th align="center">R1<br>@0.3</th>
      <th align="center">R1<br>@0.5</th>
      <th align="center">R1<br>@0.7</th>
      <th align="center">mIoU</th>
      <th align="center">R1<br>@0.3</th>
      <th align="center">R1<br>@0.5</th>
      <th align="center">R1<br>@0.7</th>
      <th align="center">mIoU</th>
      <th align="center">R1<br>@0.3</th>
      <th align="center">R1<br>@0.5</th>
      <th align="center">R1<br>@0.7</th>
      <th align="center">mIoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">Qwen2.5-VL-7B-Instruct</a></td>
      <td align="center">59.7</td>
      <td align="center">37.8</td>
      <td align="center">16.6</td>
      <td align="center">39.3</td>
      <td align="center">44.1</td>
      <td align="center">31.0</td>
      <td align="center">16.1</td>
      <td align="center">31.4</td>
      <td align="center">41.5</td>
      <td align="center">27.8</td>
      <td align="center">15.2</td>
      <td align="center">31.6</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/TencentARC/TimeLens-7B"><b>TimeLens-7B</b>ğŸš€</a></td>
      <td align="center"><b>70.5</b></td>
      <td align="center"><b>55.6</b></td>
      <td align="center"><b>28.4</b></td>
      <td align="center"><b>48.8</b></td>
      <td align="center"><b>62.8</b></td>
      <td align="center"><b>51.0</b></td>
      <td align="center"><b>32.6</b></td>
      <td align="center"><b>46.2</b></td>
      <td align="center"><b>74.1</b></td>
      <td align="center"><b>62.7</b></td>
      <td align="center"><b>43.1</b></td>
      <td align="center"><b>56.0</b></td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct">Qwen3-VL-8B-Instruct</a></td>
      <td align="center">69.2</td>
      <td align="center">53.4</td>
      <td align="center">27.5</td>
      <td align="center">48.3</td>
      <td align="center">62.1</td>
      <td align="center">51.2</td>
      <td align="center">34.4</td>
      <td align="center">46.8</td>
      <td align="center">74.2</td>
      <td align="center">64.6</td>
      <td align="center">49.3</td>
      <td align="center">59.4</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/TencentARC/TimeLens-8B"><b>TimeLens-8B</b>ğŸš€</a></td>
      <td align="center"><b>72.0</b></td>
      <td align="center"><b>56.3</b></td>
      <td align="center"><b>29.2</b></td>
      <td align="center"><b>50.3</b></td>
      <td align="center"><b>64.5</b></td>
      <td align="center"><b>53.7</b></td>
      <td align="center"><b>35.2</b></td>
      <td align="center"><b>48.7</b></td>
      <td align="center"><b>75.6</b></td>
      <td align="center"><b>65.3</b></td>
      <td align="center"><b>51.3</b></td>
      <td align="center"><b>61.5</b></td>
    </tr>
  </tbody>
</table>

> **Note:** TimeLens-7B is fine-tuned from Qwen2.5-VL-7B-Instruct, and TimeLens-8B is fine-tuned from Qwen3-VL-8B-Instruct.


## ğŸ“Š Evaluation on TimeLens-Bench

### Download TimeLens-Bench

Download the [TimeLens-Bench dataset](https://huggingface.co/datasets/TencentARC/TimeLens-Bench) from Hugging Face and place it in the `data/TimeLens-Bench` directory:
```bash
hf download TencentARC/TimeLens-Bench \
  --repo-type=dataset \
  --local-dir data/TimeLens-Bench
```

Extract the compressed videos:
```bash
mkdir -p data/TimeLens-Bench/videos
find data/TimeLens-Bench/video_shards -name "*.tar.gz" | \
  xargs -P 4 -I {} tar -xzf {} -C data/TimeLens-Bench/videos # Parallel extraction with 4 processes
```

The folder structure should look like this:
```
TimeLens/
â””â”€â”€ data/
    â””â”€â”€ TimeLens-Bench/
        â”œâ”€â”€ activitynet-timelens.json
        â”œâ”€â”€ charades-timelens.json
        â”œâ”€â”€ qvhighlights-timelens.json
        â”œâ”€â”€ videos/              # extracted videos
        â”‚   â”œâ”€â”€ activitynet/
        â”‚   â”œâ”€â”€ charades/
        â”‚   â””â”€â”€ qvhighlights/
        â””â”€â”€ video_shards/        # compressed videos (can be deleted after extraction)
```

### Evaluate with Our Codebase (TimeLens / Qwen-VL Models)

Our codebase supports evaluation of the following models:

| Model | Supported |
|:----------:|:---------:|
| [TimeLens-7B](https://huggingface.co/TencentARC/TimeLens-7B) | âœ… |
| [TimeLens-8B](https://huggingface.co/TencentARC/TimeLens-8B) | âœ… |
| [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl) | âœ… |
| [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl) | âœ… |

The evaluation script is [`scripts/eval_timelens_bench.sh`](./scripts/eval_timelens_bench.sh). You can set the following environment variables:
- **`model_path`**: Path or HuggingFace ID of the model to evaluate. Default: `TencentARC/TimeLens-8B`
- **`datasets`**: Comma-separated list of datasets to evaluate. Default: `charades-timelens,activitynet-timelens,qvhighlights-timelens`
- **`CUDA_VISIBLE_DEVICES`**: GPU indices to use (e.g., `0,1,2,3`). Default: Auto-detect all available GPUs
- **`pred_path`**: Directory to save results. Default: `./logs`
- **`min_tokens`**: Minimum tokens for video encoding. Default: `64`
- **`total_tokens`**: Total tokens for video encoding. Default: `14336`
- **`FPS`**: Frames per second for video sampling. Default: `2`

**Example 1**: Evaluate TimeLens-8B (default settings)
```bash
model_path="TencentARC/TimeLens-8B" bash scripts/eval_timelens_bench.sh
```

**Example 2**: Evaluate TimeLens-7B on specific datasets with specific GPUs
```bash
CUDA_VISIBLE_DEVICES=0,1 \
datasets="activitynet-timelens,qvhighlights-timelens" \
model_path="TencentARC/TimeLens-7B" \
bash scripts/eval_timelens_bench.sh
```

**Example 3**: Evaluate Qwen3-VL with a local model path and a custom path to save results:
```bash
pred_path="./path/to/results" \
model_path="path/to/Qwen3-VL-8B-Instruct" \
bash scripts/eval_timelens_bench.sh
```

> [!TIP]
> **Faster Evaluation with DataLoader** ğŸš€
>
> Our evaluation script [evaluation/eval_dataloader.py](./evaluation/eval_dataloader.py) supports multi-GPU inference. More importantly, we use [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html) with multiple workers to prefetch and preprocess video data in parallel, while the GPU handles model inference. This significantly accelerates evaluation for long-video tasks like video temporal grounding. Additionally, this approach is more **research-friendly** compared to inference engines like vLLM, as it allows easy customization of the model inference code.
>
> Evaluating TimeLens-7B on ActivityNet-TimeLens with 8Ã— H20 GPUs:
>
> | Method | Time |
> |:------:|:----:|
> | Without DataLoader | 1h23min |
> | With DataLoader | **~34min (~2.4x faster)** |


### Evaluate Your Own Model

To evaluate your own model on TimeLens-Bench, follow these steps:

1. **Load annotations**: Use our provided [timelens_data.py](./timelens/dataset/timelens_data.py) for loading annotations.

2. **Run inference and save results**: Run inference with your model and save results in a JSON or JSONL file with the following format:

   ```python
   {
       f'{video_name}>>>{query}>>>{ground_truth_span}': {
           "timestamps": timestamps,  # the predicted time span from the model
           "answers": answer,  # the full answer text from the model
       }
   }
   ```

   An example of a correctly saved JSON file:

   ```json
   {
       "v_BrgYIg6UXhU.mp4>>>A man wearing a blue jacket approaches a blue car>>>[0.0, 4.0]":
       {
           "timestamps": [[0.0, 5.0]],
           "answers": "The event happens in 0.0 - 5.0 seconds."
       },
       ...
   }
   ```

    In your inference results, you can provide **either** `timestamps` or `answers`. In the next step (Step 3, compute metrics), `evaluation/compute_metrics.py` applies the following logic:
      - If `timestamps` is provided, IoU metrics are computed directly from it.
   - If only `answers` is provided, the script will automatically extract the timestamp pair from the answer text.

3. **Compute metrics**: Use our provided [evaluation/compute_metrics.py](./evaluation/compute_metrics.py) to compute metrics.

  ```bash
  python evaluation/compute_metrics.py -f /path/to/your_result.json
  ```
> For more details on implementing the above steps, you can refer to the [evaluation scripts](#evaluate-with-our-codebase-timelens--qwen-vl-models) of our supported models.


## ğŸ‹ï¸ Training on TimeLens-100K

### Download TimeLens-100K

Download the [TimeLens-100K dataset](https://huggingface.co/datasets/TencentARC/TimeLens-100K) from Hugging Face and place it in the `data/TimeLens-100K` directory:
```bash
hf download TencentARC/TimeLens-100K \
  --repo-type=dataset \
  --local-dir data/TimeLens-100K
```

Extract the compressed videos:
```bash
mkdir -p data/TimeLens-100K/videos
find data/TimeLens-100K/video_shards -name "*.tar.gz" | \
  xargs -P 4 -I {} tar -xzf {} -C data/TimeLens-100K/videos # Parallel extraction with 4 processes
```

The folder structure should look like this:
```
TimeLens/
â””â”€â”€ data/
    â””â”€â”€ TimeLens-100K/
        â”œâ”€â”€ README.md
        â”œâ”€â”€ timelens-100k.jsonl
        â”œâ”€â”€ videos/              # extracted videos
        â”‚   â”œâ”€â”€ cosmo_cap/
        â”‚   â”œâ”€â”€ didemo/
        â”‚   â”œâ”€â”€ hirest/
        â”‚   â”œâ”€â”€ internvid_vtime/
        â”‚   â””â”€â”€ queryd/
        â””â”€â”€ video_shards/        # compressed videos (can be deleted after extraction)
```

### Train with Your Own Codebase

We provide an example script [timelens_data.py](./timelens/dataset/timelens_data.py) for loading TimeLens-100K annotations. You can refer to this code to integrate TimeLens-100K into your own training codebase.

### Use Our Training Code

Our training code will be released soon! Stay tuned!

## ğŸ“ Citation
If you find our paper, code, model, and data helpful for your research and applications, please consider giving a star â­ and citation ğŸ“ :)

```bibtex
TODO
```

## ğŸ™ Acknowledgement

Our project is built upon the following awesome works:

- [VideoMind](https://github.com/yeliudev/VideoMind)
- [Qwen3-VL and Qwen2.5-VL](https://github.com/QwenLM/Qwen3-VL)
- [Qwen-VL-Series-Finetune](https://github.com/2U1/Qwen-VL-Series-Finetune)
- [TRL - Transformer Reinforcement Learning](https://github.com/huggingface/trl)
- [transformers](https://github.com/huggingface/transformers)
